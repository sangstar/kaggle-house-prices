{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df = pd.read_csv('files/train.csv')\n",
    "df = df.sample(frac = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "count_nan = len(df) - df.count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df.describe()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                Id   MSSubClass  LotFrontage        LotArea  OverallQual  \\\n",
       "count  1460.000000  1460.000000  1201.000000    1460.000000  1460.000000   \n",
       "mean    730.500000    56.897260    70.049958   10516.828082     6.099315   \n",
       "std     421.610009    42.300571    24.284752    9981.264932     1.382997   \n",
       "min       1.000000    20.000000    21.000000    1300.000000     1.000000   \n",
       "25%     365.750000    20.000000    59.000000    7553.500000     5.000000   \n",
       "50%     730.500000    50.000000    69.000000    9478.500000     6.000000   \n",
       "75%    1095.250000    70.000000    80.000000   11601.500000     7.000000   \n",
       "max    1460.000000   190.000000   313.000000  215245.000000    10.000000   \n",
       "\n",
       "       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  ...  \\\n",
       "count  1460.000000  1460.000000   1460.000000  1452.000000  1460.000000  ...   \n",
       "mean      5.575342  1971.267808   1984.865753   103.685262   443.639726  ...   \n",
       "std       1.112799    30.202904     20.645407   181.066207   456.098091  ...   \n",
       "min       1.000000  1872.000000   1950.000000     0.000000     0.000000  ...   \n",
       "25%       5.000000  1954.000000   1967.000000     0.000000     0.000000  ...   \n",
       "50%       5.000000  1973.000000   1994.000000     0.000000   383.500000  ...   \n",
       "75%       6.000000  2000.000000   2004.000000   166.000000   712.250000  ...   \n",
       "max       9.000000  2010.000000   2010.000000  1600.000000  5644.000000  ...   \n",
       "\n",
       "        WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  ScreenPorch  \\\n",
       "count  1460.000000  1460.000000    1460.000000  1460.000000  1460.000000   \n",
       "mean     94.244521    46.660274      21.954110     3.409589    15.060959   \n",
       "std     125.338794    66.256028      61.119149    29.317331    55.757415   \n",
       "min       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "50%       0.000000    25.000000       0.000000     0.000000     0.000000   \n",
       "75%     168.000000    68.000000       0.000000     0.000000     0.000000   \n",
       "max     857.000000   547.000000     552.000000   508.000000   480.000000   \n",
       "\n",
       "          PoolArea       MiscVal       MoSold       YrSold      SalePrice  \n",
       "count  1460.000000   1460.000000  1460.000000  1460.000000    1460.000000  \n",
       "mean      2.758904     43.489041     6.321918  2007.815753  180921.195890  \n",
       "std      40.177307    496.123024     2.703626     1.328095   79442.502883  \n",
       "min       0.000000      0.000000     1.000000  2006.000000   34900.000000  \n",
       "25%       0.000000      0.000000     5.000000  2007.000000  129975.000000  \n",
       "50%       0.000000      0.000000     6.000000  2008.000000  163000.000000  \n",
       "75%       0.000000      0.000000     8.000000  2009.000000  214000.000000  \n",
       "max     738.000000  15500.000000    12.000000  2010.000000  755000.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1452.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>56.897260</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>10516.828082</td>\n",
       "      <td>6.099315</td>\n",
       "      <td>5.575342</td>\n",
       "      <td>1971.267808</td>\n",
       "      <td>1984.865753</td>\n",
       "      <td>103.685262</td>\n",
       "      <td>443.639726</td>\n",
       "      <td>...</td>\n",
       "      <td>94.244521</td>\n",
       "      <td>46.660274</td>\n",
       "      <td>21.954110</td>\n",
       "      <td>3.409589</td>\n",
       "      <td>15.060959</td>\n",
       "      <td>2.758904</td>\n",
       "      <td>43.489041</td>\n",
       "      <td>6.321918</td>\n",
       "      <td>2007.815753</td>\n",
       "      <td>180921.195890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>421.610009</td>\n",
       "      <td>42.300571</td>\n",
       "      <td>24.284752</td>\n",
       "      <td>9981.264932</td>\n",
       "      <td>1.382997</td>\n",
       "      <td>1.112799</td>\n",
       "      <td>30.202904</td>\n",
       "      <td>20.645407</td>\n",
       "      <td>181.066207</td>\n",
       "      <td>456.098091</td>\n",
       "      <td>...</td>\n",
       "      <td>125.338794</td>\n",
       "      <td>66.256028</td>\n",
       "      <td>61.119149</td>\n",
       "      <td>29.317331</td>\n",
       "      <td>55.757415</td>\n",
       "      <td>40.177307</td>\n",
       "      <td>496.123024</td>\n",
       "      <td>2.703626</td>\n",
       "      <td>1.328095</td>\n",
       "      <td>79442.502883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "      <td>34900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>365.750000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7553.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>1967.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>129975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>9478.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>383.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>163000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1095.250000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11601.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>712.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>214000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>5644.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>857.000000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>15500.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>755000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 38 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Check for NaN columns..\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "for cols in df.columns:\n",
    "    if df[cols].isnull().any():\n",
    "        print(cols)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LotFrontage\n",
      "Alley\n",
      "MasVnrType\n",
      "MasVnrArea\n",
      "BsmtQual\n",
      "BsmtCond\n",
      "BsmtExposure\n",
      "BsmtFinType1\n",
      "BsmtFinType2\n",
      "Electrical\n",
      "FireplaceQu\n",
      "GarageType\n",
      "GarageYrBlt\n",
      "GarageFinish\n",
      "GarageQual\n",
      "GarageCond\n",
      "PoolQC\n",
      "Fence\n",
      "MiscFeature\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Let's go through how I might want to treat each column, before I get into dealing with NaN's..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MSSubClass\n",
    "\n",
    "The values it takes are kind of weird considering its categorical data. If I decide to use a decision tree-based algorithm then it's fine. If not I'll need to at least normailize this one."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MSZoning \n",
    "Should be one-hot encoded."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LotFrontage, LotArea\n",
    "Numerical data. We're dealing with a bunch of different units here, and if I wasn't working with a decision tree I'd really need to be careful to normalize and standardize the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Street, Alley, LandContour, LotConfig, Neighborhood, Condition1, Condition2, BldgType, HouseStyle\n",
    "Categorical. To be one-hot encoded."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## OverallCond and OverallQual\n",
    "Numerical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## YearBuilt, YearRemodAdd\n",
    "Numerical data. Need to think about how I should treat this. Could maybe base them off of how many days ago they were built / remodeled."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RoofStyle, RoofMatl, Exterior1st, Exterior2nd, MasVnrType\n",
    "All categorical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MasVnrArea\n",
    "Numerical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LotShape, Utilities, LandSlope, ExterQual, ExterCond\n",
    "Categorical, but admits an ordered set, with Excellent > Good > Average/Typical etc.. so can convert to numerical to save on columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Foundation\n",
    "Categorical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1\n",
    "Categorical, but again admitting an ordered set, so should be converted into numerical data to save on columns."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BsmtFinSF1\n",
    "Numerical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BsmtFinType2\n",
    "Categorical, but admitting an ordered set -- convert to numeric."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BsmtFinSF2, BsmtUnfSF, TotalBsmtSF\n",
    "Numerical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Heating\n",
    "Categorical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HeatingQC\n",
    "Categorical, but ordered set so convert to numeric."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CentralAir, Electrical\n",
    "Categorical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1stFlrSF, 2ndFlrSF, LowQualFinSF, GrLivArea, BsmtFullBath,  BsmtHalfBath, FullBath, HalfBath, Bedroom, Kitchen\n",
    "Numerical"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KitchenQual\n",
    "Categoric, but admits ordered set, so convert to numerical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TotRmsAbvGrd\n",
    "Numerical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functional\n",
    "Categorical, but admits ordered set. Convert to numerical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fireplaces\n",
    "Numerical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FireplaceQu\n",
    "Categorical -- convertable to numerical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GarageType\n",
    "Categorical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GarageFinish\n",
    "Categorical -- convertable to numerical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GarageYrBlt\n",
    "Possibly convert to 'how many days ago'."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GarageCars, GarageArea\n",
    "Numerical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GarageQual, GarageCond\n",
    "Categorical -- convertable to numerical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PavedDrive\n",
    "I think this admits an ordered set in terms of 'paved-ness'. So possibly convertable to numerical, although could be safe and just keep it categorical -- only like 3 unique values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## WoodDeckSF, OpenPorchSF, EnclosedPorch, 3SsnPorch, ScreenPorch, PoolArea\n",
    "Numerical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PoolQC, Fence\n",
    "Categorical -- convertable to numerical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MiscFeature\n",
    "Categorical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MiscVal\n",
    "Numerical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MoSold , YrSold\n",
    "MoSold might not be worth the effort to include. I won't want to one-hot encode 12 columns for each month, and I doubt the month something was sold can be worth the 12 columns I'd be costing the training data. YrSold I can express in terms of 'days since the present'. I could maybe express MoSold as numerical data of values between 1 and 12. But I don't think that's justifiable because it's weird to say that month 2 > month 1 or something."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SaleType, SaleCondition\n",
    "Categorical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Okay, with that, here are my next steps:\n",
    "\n",
    "There isn't a whole lot of rows to this data, so if I add too many columns we start worrying about big-p, little-n issues. I need to try and impute my NaN's instead of deleting those rows so that the little-n doesn't get even smaller. I'll also really want to favor converting categorical data that admits an ordered set into numerical data so I can save up on columns.\n",
    "1. Create a function to deal with missing NaN values sample-by-sample by selecting a subset of the data that shares as many characteristics as possible with a sample and picking the most likely value to impute given this. \n",
    "2. Create a function that converts categorical-but-numerical-convertable columns into numeric columns.\n",
    "3. Deal with time-series columns."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It might be worth trying to find similar columns by one-hot encoding and normalizing data, and seeing which samples have the highest cosine similarity to the one in question.. I'd need to normalize numerical data, or else different magnitudes would have disproportionate influence on the the cosine similarity.."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# One-hot encoding categorical data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also going to drop the MoSold column."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df = df.drop(columns = ['MoSold'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "to_one_hot = [\n",
    "    'MiscFeature',\n",
    "    'SaleType',\n",
    "    'SaleCondition',\n",
    "    'CentralAir', \n",
    "    'Electrical',\n",
    "    'Heating',\n",
    "    'Foundation',\n",
    "    'RoofStyle', \n",
    "    'RoofMatl', \n",
    "    'Exterior1st', \n",
    "    'Exterior2nd', \n",
    "    'MasVnrType',\n",
    "    'Street', \n",
    "    'Alley', \n",
    "    'LandContour', \n",
    "    'LotConfig', \n",
    "    'Neighborhood', \n",
    "    'Condition1', \n",
    "    'Condition2', \n",
    "    'BldgType', \n",
    "    'HouseStyle',\n",
    "    'MSZoning',\n",
    "    'MSSubClass',\n",
    "    'GarageType',\n",
    "    'PavedDrive'\n",
    "    ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "time_series_columns = ['YrSold','YearBuilt','YearRemodAdd','GarageYrBlt']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now for the data I want to make numeric."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "numerical = [\n",
    "    'MiscVal',\n",
    "    'WoodDeckSF', \n",
    "    'OpenPorchSF', \n",
    "    'EnclosedPorch', \n",
    "    '3SsnPorch', \n",
    "    'ScreenPorch', \n",
    "    'PoolArea',\n",
    "    'GarageCars', \n",
    "    'GarageArea',\n",
    "    'Fireplaces',\n",
    "    'TotRmsAbvGrd',\n",
    "    '1stFlrSF', \n",
    "    '2ndFlrSF', \n",
    "    'LowQualFinSF', \n",
    "    'GrLivArea', \n",
    "    'BsmtFullBath',  \n",
    "    'BsmtHalfBath', \n",
    "    'FullBath', \n",
    "    'HalfBath', \n",
    "    'BedroomAbvGr', \n",
    "    'KitchenAbvGr',\n",
    "    'BsmtFinSF2', \n",
    "    'BsmtUnfSF', \n",
    "    'TotalBsmtSF',\n",
    "    'BsmtFinSF1',\n",
    "    'MasVnrArea',\n",
    "    'LotFrontage', \n",
    "    'LotArea',\n",
    "    'OverallCond',\n",
    "    'OverallQual',\n",
    "      \n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "columns_so_far = to_one_hot + time_series_columns + numerical"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "full_columns = list(df.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "to_numerical = list(set(columns_so_far).symmetric_difference(full_columns))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "to_numerical.remove('SalePrice')\n",
    "to_numerical.remove(\"Id\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "to_numerical"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['LandSlope',\n",
       " 'GarageFinish',\n",
       " 'ExterQual',\n",
       " 'GarageCond',\n",
       " 'ExterCond',\n",
       " 'Utilities',\n",
       " 'FireplaceQu',\n",
       " 'KitchenQual',\n",
       " 'BsmtQual',\n",
       " 'HeatingQC',\n",
       " 'PoolQC',\n",
       " 'BsmtFinType2',\n",
       " 'BsmtFinType1',\n",
       " 'Functional',\n",
       " 'GarageQual',\n",
       " 'Fence',\n",
       " 'LotShape',\n",
       " 'BsmtExposure',\n",
       " 'BsmtCond']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## One-hot encode the categorical columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "df = pd.get_dummies(df,columns = to_one_hot)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert the to_numerical data to numerical"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import json \n",
    "with open('files/to_numerical_json.json', 'r') as f:   \n",
    "    ordered_categories = json.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "ordered_categories"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Fence': ['GdPrv', 'MnPrv', 'GdWo', 'MnWw', 'NA'],\n",
       " 'LotShape': ['Reg', 'IR1', 'IR2', 'IR3'],\n",
       " 'Utilities': ['AllPub', 'NoSewr', 'NoSeWa', 'ELO'],\n",
       " 'LandSlope': ['Gtl', 'Mod', 'Sev'],\n",
       " 'BsmtQual': ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'NA'],\n",
       " 'BsmtCond': ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'NA'],\n",
       " 'BsmtExposure': ['Gd', 'Av', 'Mn', 'No', 'NA'],\n",
       " 'BsmtFinType1': ['GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ', 'Unf', 'NA'],\n",
       " 'BsmtFinType2': ['GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ', 'Unf', 'NA'],\n",
       " 'HeatingQC': ['Ex', 'Gd', 'TA', 'Fa', 'Po'],\n",
       " 'KitchenQual': ['Ex', 'Gd', 'TA', 'Fa', 'Po'],\n",
       " 'Functional': ['Typ', 'Min1', 'Min2', 'Mod', 'Maj1', 'Maj2', 'Sev', 'Sal'],\n",
       " 'FireplaceQu': ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'NA'],\n",
       " 'GarageFinish': ['Fin', 'RFn', 'Unf', 'NA'],\n",
       " 'ExterCond': ['Ex', 'Gd', 'TA', 'Fa', 'Po'],\n",
       " 'ExterQual': ['Ex', 'Gd', 'TA', 'Fa', 'Po'],\n",
       " 'GarageQual': ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'NA'],\n",
       " 'GarageCond': ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'NA'],\n",
       " 'PoolQC': ['Ex', 'Gd', 'TA', 'Fa', 'NA']}"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from funcs.conv_to_numerical import conv_to_numerical"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "df = conv_to_numerical(df, ordered_categories)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert dates into 'days ago'\n",
    "Going to assume all houses listed for a given year were listed on January 1st of that year. Because I'm omitting month, the time during the year won't matter."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from funcs.get_days_ago import get_days_ago"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "for cols in time_series_columns:\n",
    "    df[cols] = [get_days_ago(year) for year in df[cols]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Normalize the numerical columns for the cosine similarity steps\n",
    "Normalizing is not generally a good idea for a decision tree-based model, so I'll probably make it separate to df. I'll also add the time-series columns to the numerical data now that I've converted it to numerical data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "all_numerical = to_numerical + numerical + time_series_columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "y = df['SalePrice']\n",
    "df = df.drop(columns = ['SalePrice'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#cos_sim_df = cos_sim_df.drop(columns = numerical)\n",
    "df[all_numerical] = min_max_scaler.fit_transform(df[all_numerical])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Now to implement the cosine similarity function to find a subset of data most similar to a sample with one or more NaN values. Just need to find the cosine similarity between the sample with NaN of interest with all other columns, dealing with NaN's in the cosine similarity (either make the similarity zero or NaN or something) and get a subset of the data which has a cosine similarity score above a certain threshold. From that subset, take the mean/median of the values of the subset to fill in the NaN for that sample."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "from funcs.get_most_similar_rows import get_most_similar_rows\n",
    "\n",
    "results = get_most_similar_rows(df)\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nan nan nan]\n",
      "[nan nan]\n",
      "[nan nan nan nan nan nan]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "One of the values is not NaN",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Downloads/house-prices-advanced-regression-techniques/funcs/get_most_similar_rows.py\u001b[0m in \u001b[0;36mget_most_similar_rows\u001b[0;34m(cos_sim_df)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"files/most_sim_rows.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'files/most_sim_rows.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gv/zxch4f5j72d3kxhj77v41_l00000gn/T/ipykernel_46223/149562889.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_most_similar_rows\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_most_similar_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_most_similar_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/house-prices-advanced-regression-techniques/funcs/get_most_similar_rows.py\u001b[0m in \u001b[0;36mget_most_similar_rows\u001b[0;34m(cos_sim_df)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnan_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcos_sim_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos_sim_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_of_nan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'One of the values is not NaN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0mpermitted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_with_nan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermitted_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_with_nan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: One of the values is not NaN"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## My NaN-detecting code is not working properly. Need to make sure this works "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with  open(\"files/most_sim_rows.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'33.0' in list(results.keys())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.isnull().sum(axis = 1).sum()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4100"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'617.0' in list(results.keys())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# In order to get the 'best' subset, take the top nth (maybe 70th?) percentile of cos_sim scores as the subset. Then, just form a subset of df \n",
    "# with those indices that pass.\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "cutoff = 0.75\n",
    "alpha = 1e-3\n",
    "\n",
    "\n",
    "for indices in df['Id']:\n",
    "    #print(indices in list(results.keys()))\n",
    "    indices = float(indices)\n",
    "    #print(results[indices])\n",
    "    #try:\n",
    "    #print(indices in list(results.keys()))\n",
    "    to_compare = [int(float(x)) for x in list(results.keys())]\n",
    "    if indices in to_compare:\n",
    "        #print(results_indices)\n",
    "        indices_as_str = str(indices)\n",
    "        most_similar = [x[0] for x in results[indices_as_str] if x[1] >= cutoff]\n",
    "        columns_to_fill = [x[2] for x in results[indices_as_str] if x[1] >= cutoff]\n",
    "        subset = df.iloc[most_similar]\n",
    "        #print(columns_to_fill)\n",
    "        columns_to_fill = list(set(tuple(row) for row in columns_to_fill))\n",
    "        #print('columns to fill:', columns_to_fill)\n",
    "        for col_list in columns_to_fill: # Don't want to use 'mean' on categorical data\n",
    "            for col in col_list:\n",
    "                print(indices, col)\n",
    "                col_name = df.columns[col]\n",
    "                print('before at index',indices,'and column', col, col_name,'sample is ',df.loc[indices, col_name])\n",
    "                if not pd.isnull(df.loc[indices, col_name]):\n",
    "                    raise ValueError(df.loc[indices, col_name], 'should be null')\n",
    "                subset_test = subset[col_name]\n",
    "                if col in all_numerical:\n",
    "                    if len(subset_test) >= 8: # normaltest requires at least 8 samples\n",
    "                        results = normaltest(subset_test)\n",
    "                        if results[1] > alpha: #null hypothesis cannot be rejected that distribution is normal. Take mean of gaussian\n",
    "                            df.loc[indices, col_name] = np.nanmean(subset_test)\n",
    "                        elif results[1] < alpha:\n",
    "                            df.loc[indices, col_name] = np.nanmedian(subset_test)\n",
    "                    else:\n",
    "                        print('less than 8 samples')\n",
    "                        mean = np.nanmean(subset_test)\n",
    "                        median = np.nanmedian(subset_test)\n",
    "                        print(mean, median)\n",
    "                        if np.abs((mean-median)/mean)*100 < 10: # Mean and median are close together -- can pick either, will pick mean\n",
    "                            df.loc[indices, col_name] = mean\n",
    "                        else: # Pick median -- not close together possibly due to outliers\n",
    "                            df.loc[indices, col_name] = media\n",
    "                else:\n",
    "                    df.loc[indices, col_name] = np.nanmedian(subset_test)\n",
    "                print('after at index',indices,'and column', col,'sample is ',df.loc[indices, col_name])\n",
    "    #except KeyError:\n",
    "    #    print('KeyError')\n",
    "    #    #print('No NaN for index', indices, 'passing..')\n",
    "    #    pass"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1128.0 50\n",
      "before at index 1128.0 and column 50 PoolQC sample is  nan\n",
      "after at index 1128.0 and column 50 sample is  nan\n",
      "535.0 50\n",
      "before at index 535.0 and column 50 PoolQC sample is  nan\n",
      "after at index 535.0 and column 50 sample is  0.6666666666666667\n",
      "535.0 51\n",
      "before at index 535.0 and column 51 Fence sample is  nan\n",
      "after at index 535.0 and column 51 sample is  0.6666666666666667\n",
      "242.0 50\n",
      "before at index 242.0 and column 50 PoolQC sample is  nan\n",
      "after at index 242.0 and column 50 sample is  nan\n",
      "1093.0 50\n",
      "before at index 1093.0 and column 50 PoolQC sample is  nan\n",
      "after at index 1093.0 and column 50 sample is  0.6666666666666667\n",
      "1093.0 51\n",
      "before at index 1093.0 and column 51 Fence sample is  0.6666666666666667\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/sangersteel/miniforge3/envs/toxic/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1113: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "(0.6666666666666667, 'should be null')",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gv/zxch4f5j72d3kxhj77v41_l00000gn/T/ipykernel_45517/1485550362.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before at index'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'and column'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sample is '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'should be null'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0msubset_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_numerical\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: (0.6666666666666667, 'should be null')"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.isnull().any().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The method above runs the risk of getting values like 0.5 for categorical columns. Can fix this by just enforcing the median always. "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('toxic': conda)"
  },
  "interpreter": {
   "hash": "c0ac4971e360b0f2dea732e840eb0c9c41456636b7685f248e181635d8e4811b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}